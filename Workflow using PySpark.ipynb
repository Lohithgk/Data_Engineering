{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21149680-559d-4800-8715-5b86d88cb16e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --------------------------------------\n",
    "# 1. Imports PySpark’s types & Functions\n",
    "# --------------------------------------\n",
    "# Imports PySpark’s types for defining structured schemas and the col/sum functions for DataFrame transformations\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DateType, DoubleType\n",
    "from pyspark.sql.functions import col, sum\n",
    "\n",
    "# ---------------------------\n",
    "# 2. Initialize Spark Session\n",
    "# ---------------------------\n",
    "# In Databricks, a SparkSession is automatically created when we start a notebook, so you don’t need to manually create one. \n",
    "# The default SparkSession is available as spark\n",
    "\n",
    "spark = (SparkSession\n",
    "         .builder\n",
    "         .appName(\"DataflowPySpark\")\n",
    "         .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c7218a1a-208f-4d6b-9d6f-1ca2b6a1dd37",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------\n",
    "# 1. Read Sales.csv file located in the distributed file system\n",
    "# -------------------------------------------------------------\n",
    "# 1. File locations and type\n",
    "sales_file_location = \"dbfs:/FileStore/tables/Sales.csv\"\n",
    "file_type = \"csv\"\n",
    "\n",
    "# 2. CSV options\n",
    "infer_schema = \"False\"\n",
    "first_row_is_header = \"True\"\n",
    "delimiter = \",\"\n",
    "\n",
    "# 3. Define the Schema for Sales File\n",
    "sales_schema = StructType([ StructField('OrderDate', DateType(), True),\n",
    "                            StructField('StockDate', DateType(), True),\n",
    "                            StructField('OrderNumber', StringType(), True),\n",
    "                            StructField('ProductKey', IntegerType(), True),\n",
    "                            StructField('CustomerKey', IntegerType(), True),\n",
    "                            StructField('TerritoryKey', IntegerType(), True),\n",
    "                            StructField('OrderLineItem', IntegerType(), True),\n",
    "                            StructField('OrderQuantity', IntegerType(), True)])\n",
    "\n",
    "# 4. Read the data from a CSV file, with user defined schema into a dataframe Sales_df.\n",
    "Sales_df = (spark\n",
    "            .read\n",
    "            .format(file_type)\n",
    "            .option(\"inferSchema\", infer_schema)\n",
    "            .schema(sales_schema)\n",
    "            .option(\"header\", first_row_is_header)\n",
    "            .option(\"sep\", delimiter)\n",
    "            .load(sales_file_location))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8fda234-b5f3-40f5-9170-e05ae19bb363",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------\n",
    "# 2. Read Customers.csv file located in the distributed file system\n",
    "# -----------------------------------------------------------------\n",
    "# 1. File location\n",
    "customers_file_location = \"dbfs:/FileStore/tables/Customers.csv\"\n",
    "\n",
    "# 2. CSV options\n",
    "infer_schema = \"False\"\n",
    "first_row_is_header = \"True\"\n",
    "delimiter = \",\"\n",
    "\n",
    "# 3. Define the Schema for customers File\n",
    "customers_schema = StructType([ StructField('CustomerKey', IntegerType(), nullable=False),\n",
    "                                StructField('Prefix', StringType(), True),\n",
    "                                StructField('FirstName', StringType(), True),\n",
    "                                StructField('LastName', StringType(), True),\n",
    "                                StructField('BirthDate', DateType(), True),\n",
    "                                StructField('MaritalStatus', StringType(), True),\n",
    "                                StructField('Gender', StringType(), True),\n",
    "                                StructField('EmailAddress', StringType(), True),\n",
    "                                StructField('AnnualIncome', StringType(), True),\n",
    "                                StructField('TotalChildren', IntegerType(), True),\n",
    "                                StructField('EducationLevel', StringType(), True),\n",
    "                                StructField('Occupation', StringType(), True),\n",
    "                                StructField('HomeOwner', StringType(), True)])\n",
    "\n",
    "# 4. Read the data from a CSV file, with user defined schema into a dataframe Customers_df\n",
    "Customers_df = (spark\n",
    "                .read\n",
    "                .format(file_type)\n",
    "                .option(\"inferSchema\", infer_schema)\n",
    "                .schema(customers_schema)\n",
    "                .option(\"header\", first_row_is_header)\n",
    "                .option(\"sep\", delimiter)\n",
    "                .load(customers_file_location))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3bb77ab8-afdd-45ac-9998-c83f288a6a35",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------\n",
    "# 3. Read Products.csv file located in the distributed file system\n",
    "# ----------------------------------------------------------------\n",
    "# 1. File location\n",
    "products_file_location = \"dbfs:/FileStore/tables/Products.csv\"\n",
    "\n",
    "# 2. CSV options\n",
    "infer_schema = \"False\"\n",
    "first_row_is_header = \"True\"\n",
    "delimiter = \",\"\n",
    "\n",
    "# 3. Define the Schema for customers File\n",
    "products_schema = StructType([StructField('ProductKey', IntegerType(), nullable=False),\n",
    "                              StructField('ProductSubcategoryKey', IntegerType(), True),\n",
    "                              StructField('ProductSKU', StringType(), True),\n",
    "                              StructField('ProductName', StringType(), True),\n",
    "                              StructField('ModelName', StringType(), True),\n",
    "                              StructField('ProductDescription', StringType(), True),\n",
    "                              StructField('ProductColor', StringType(), True),\n",
    "                              StructField('ProductSize', StringType(), True),\n",
    "                              StructField('ProductStyle', StringType(), True),\n",
    "                              StructField('ProductCost', DoubleType(), True),\n",
    "                              StructField('ProductPrice', DoubleType(), True)])\n",
    "\n",
    "# 4. Read the data from a CSV file, with user defined schema into a dataframe products_df\n",
    "products_df = (spark\n",
    "              .read\n",
    "              .format(file_type)\n",
    "              .option(\"inferSchema\", infer_schema)\n",
    "              .schema(products_schema)\n",
    "              .option(\"header\", first_row_is_header)\n",
    "              .option(\"sep\", delimiter)\n",
    "              .load(products_file_location))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a0ec111-a405-4f70-9d32-4b19953f05e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------\n",
    "# 4. Read product_categories.csv file located in the distributed file system\n",
    "# --------------------------------------------------------------------------\n",
    "# 1. File location\n",
    "product_categories_file_location = \"dbfs:/FileStore/tables/Product_Categories.csv\"\n",
    "\n",
    "# 2. CSV options\n",
    "infer_schema = \"False\"\n",
    "first_row_is_header = \"True\"\n",
    "delimiter = \",\"\n",
    "\n",
    "# 3. Define the Schema for product_categories File\n",
    "product_categories_schema = StructType([ StructField('ProductCategoryKey', IntegerType(), nullable=False),\n",
    "                                         StructField('CategoryName', StringType(), True)])\n",
    "\n",
    "# 4. Read the data from a CSV file, with user defined schema into a dataframe product_categories_df\n",
    "product_categories_df = (spark\n",
    "                          .read\n",
    "                          .format(file_type)\n",
    "                          .option(\"inferSchema\", infer_schema)\n",
    "                          .schema(product_categories_schema)\n",
    "                          .option(\"header\", first_row_is_header)\n",
    "                          .option(\"sep\", delimiter)\n",
    "                          .load(product_categories_file_location))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6af8bde7-3631-4205-af9b-ba08e86cff40",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# 5. Read product_subcategories.csv file located in the distributed file system\n",
    "# -----------------------------------------------------------------------------\n",
    "# 1. File location\n",
    "product_subcategories_file_location = \"dbfs:/FileStore/tables/Product_Subcategories.csv\"\n",
    "\n",
    "# 2. CSV options\n",
    "infer_schema = \"False\"\n",
    "first_row_is_header = \"True\"\n",
    "delimiter = \",\"\n",
    "\n",
    "# 3. Define the Schema for product_subcategories File\n",
    "product_subcategories_schema = StructType([ StructField('ProductSubcategoryKey', IntegerType(), nullable=False),\n",
    "                                            StructField('SubcategoryName', StringType(), True),\n",
    "                                            StructField('ProductCategoryKey', IntegerType(), True)])\n",
    "\n",
    "# 4. Read the data from a CSV file, with user defined schema into a dataframe product_subcategories_df\n",
    "product_subcategories_df = (spark\n",
    "                            .read\n",
    "                            .format(file_type)\n",
    "                            .option(\"inferSchema\", infer_schema)\n",
    "                            .schema(product_subcategories_schema)\n",
    "                            .option(\"header\", first_row_is_header)\n",
    "                            .option(\"sep\", delimiter)\n",
    "                            .load(product_subcategories_file_location))                      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "110044c7-1176-42bd-895a-3fbe93b25ef8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------\n",
    "# 6. Read territories.csv file located in the distributed file system\n",
    "# -------------------------------------------------------------------\n",
    "# 1. File location\n",
    "territories_file_location = \"dbfs:/FileStore/tables/Territories.csv\"\n",
    "\n",
    "# 2. CSV options\n",
    "infer_schema = \"False\"\n",
    "first_row_is_header = \"True\"\n",
    "delimiter = \",\"\n",
    "\n",
    "# 3. Define the Schema for territories_file File\n",
    "territories_schema = StructType([ StructField('SalesTerritoryKey', IntegerType(), nullable=False),\n",
    "                                  StructField('Region', StringType(), True),\n",
    "                                  StructField('Country', StringType(), True),\n",
    "                                  StructField('Continent', StringType(), True)])\n",
    "\n",
    "# 4. Read the data from a CSV file, with user defined schema into a dataframe territories_df\n",
    "territories_df = (spark\n",
    "                  .read\n",
    "                  .format(file_type)\n",
    "                  .option(\"inferSchema\", infer_schema)\n",
    "                  .schema(territories_schema)\n",
    "                  .option(\"header\", first_row_is_header)\n",
    "                  .option(\"sep\", delimiter)\n",
    "                  .load(territories_file_location))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79ed6a61-aadc-46a2-aebb-3c0756392626",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -----------------\n",
    "# 7. Transformation \n",
    "# -----------------\n",
    "# Filter the Accessories category in the United States and rollup the Sales & Quantity at the Customer-level.\n",
    "\n",
    "bike_df = (Sales_df\n",
    "          .join(other=products_df, on=['ProductKey'], how='left')\n",
    "          .join(other=product_subcategories_df, on=['ProductSubcategoryKey'], how='left')\n",
    "          .join(other=product_categories_df, on=['ProductCategoryKey'], how='left')\n",
    "          .join(other=Customers_df, on='CustomerKey', how='left')\n",
    "          .join(other=territories_df, on=[Sales_df[\"TerritoryKey\"] == territories_df[\"SalesTerritoryKey\"]], how='left')\n",
    "          .filter((col('CategoryName')=='Accessories') & (col('Country')=='United States'))\n",
    "          .groupBy('CustomerKey','FirstName','LastName','CategoryName','Country')\n",
    "          .agg(sum('ProductPrice').alias('Total_Price'), \n",
    "               sum('OrderQuantity').alias('Total_Quantity'))\n",
    "          .withColumn('Total_Sales', col('Total_Price') * col('Total_Quantity'))\n",
    "          .select('CustomerKey','FirstName','LastName','CategoryName','Country', 'Total_Sales', 'Total_Quantity'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8713d034-4632-4cbd-a5d8-80017e3451c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ------------\n",
    "# 8. Data Load\n",
    "# ------------\n",
    "# Processed Data is Loaded into the Databricks Distributed file System \n",
    "\n",
    "bike_df.coalesce(1)\\\n",
    ".write\\\n",
    ".mode(\"overwrite\")\\\n",
    ".option(\"header\", True)\\\n",
    ".option(\"compression\", \"gzip\")\\\n",
    ".csv('dbfs:/user/g.k.lohit@gmail.com')"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "Workflow using PySpark",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
