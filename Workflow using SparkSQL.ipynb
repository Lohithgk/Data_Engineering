{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21149680-559d-4800-8715-5b86d88cb16e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --------------------------------------\n",
    "# 1. Import PySpark’s types & Functions\n",
    "# --------------------------------------\n",
    "# Import PySpark’s types for defining structured schemas and the col/sum functions for DataFrame transformations\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DateType, DoubleType\n",
    "from pyspark.sql.functions import col, sum\n",
    "\n",
    "# ---------------------------\n",
    "# 2. Initialize Spark Session\n",
    "# ---------------------------\n",
    "# In Databricks, a SparkSession is automatically created when we start a notebook, so you don’t need to manually create one. \n",
    "# The default SparkSession is available as spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c7218a1a-208f-4d6b-9d6f-1ca2b6a1dd37",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------\n",
    "# 1. Read Sales.csv file located in the distributed file system\n",
    "# -------------------------------------------------------------\n",
    "# 1. File locations and type\n",
    "sales_file_location = \"dbfs:/FileStore/tables/Sales.csv\"\n",
    "file_type = \"csv\"\n",
    "\n",
    "# 2. CSV options\n",
    "infer_schema = \"False\"\n",
    "first_row_is_header = \"True\"\n",
    "delimiter = \",\"\n",
    "\n",
    "# 3. Define the Schema for Sales File\n",
    "sales_schema = StructType([ StructField('OrderDate', DateType(), True),\n",
    "                            StructField('StockDate', DateType(), True),\n",
    "                            StructField('OrderNumber', StringType(), True),\n",
    "                            StructField('ProductKey', IntegerType(), True),\n",
    "                            StructField('CustomerKey', IntegerType(), True),\n",
    "                            StructField('TerritoryKey', IntegerType(), True),\n",
    "                            StructField('OrderLineItem', IntegerType(), True),\n",
    "                            StructField('OrderQuantity', IntegerType(), True)])\n",
    "\n",
    "# 4. The applied options are for CSV files. For other file types, these will be ignored.\n",
    "Sales_df = (spark\n",
    "            .read\n",
    "            .format(file_type)\n",
    "            .option(\"inferSchema\", infer_schema)\n",
    "            .schema(sales_schema)\n",
    "            .option(\"header\", first_row_is_header)\n",
    "            .option(\"sep\", delimiter)\n",
    "            .load(sales_file_location))\n",
    "\n",
    "# 5. Register Sales_df DataFrame as a Temporary View\n",
    "Sales_df.createOrReplaceTempView(\"sales\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8fda234-b5f3-40f5-9170-e05ae19bb363",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------\n",
    "# 2. Read Customers.csv file located in the distributed file system\n",
    "# -----------------------------------------------------------------\n",
    "# 1. File location\n",
    "customers_file_location = \"dbfs:/FileStore/tables/Customers.csv\"\n",
    "\n",
    "# 2. CSV options\n",
    "infer_schema = \"False\"\n",
    "first_row_is_header = \"True\"\n",
    "delimiter = \",\"\n",
    "\n",
    "# 3. Define the Schema for customers File\n",
    "customers_schema = StructType([ StructField('CustomerKey', IntegerType(), nullable=False),\n",
    "                                StructField('Prefix', StringType(), True),\n",
    "                                StructField('FirstName', StringType(), True),\n",
    "                                StructField('LastName', StringType(), True),\n",
    "                                StructField('BirthDate', DateType(), True),\n",
    "                                StructField('MaritalStatus', StringType(), True),\n",
    "                                StructField('Gender', StringType(), True),\n",
    "                                StructField('EmailAddress', StringType(), True),\n",
    "                                StructField('AnnualIncome', StringType(), True),\n",
    "                                StructField('TotalChildren', IntegerType(), True),\n",
    "                                StructField('EducationLevel', StringType(), True),\n",
    "                                StructField('Occupation', StringType(), True),\n",
    "                                StructField('HomeOwner', StringType(), True)])\n",
    "\n",
    "# 4. Read the data from a CSV file, with user defined schema into a dataframe Customers_df\n",
    "Customers_df = (spark\n",
    "                .read\n",
    "                .format(file_type)\n",
    "                .option(\"inferSchema\", infer_schema)\n",
    "                .schema(customers_schema)\n",
    "                .option(\"header\", first_row_is_header)\n",
    "                .option(\"sep\", delimiter)\n",
    "                .load(customers_file_location))\n",
    "\n",
    "# 5. Register Customers_df DataFrame as a Temporary View\n",
    "Customers_df.createOrReplaceTempView('customers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3bb77ab8-afdd-45ac-9998-c83f288a6a35",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------\n",
    "# 3. Read Products.csv file located in the distributed file system\n",
    "# ----------------------------------------------------------------\n",
    "# 1. File location\n",
    "products_file_location = \"dbfs:/FileStore/tables/Products.csv\"\n",
    "\n",
    "# 2. CSV options\n",
    "infer_schema = \"False\"\n",
    "first_row_is_header = \"True\"\n",
    "delimiter = \",\"\n",
    "\n",
    "# 3. Define the Schema for customers File\n",
    "products_schema = StructType([StructField('ProductKey', IntegerType(), nullable=False),\n",
    "                              StructField('ProductSubcategoryKey', IntegerType(), True),\n",
    "                              StructField('ProductSKU', StringType(), True),\n",
    "                              StructField('ProductName', StringType(), True),\n",
    "                              StructField('ModelName', StringType(), True),\n",
    "                              StructField('ProductDescription', StringType(), True),\n",
    "                              StructField('ProductColor', StringType(), True),\n",
    "                              StructField('ProductSize', StringType(), True),\n",
    "                              StructField('ProductStyle', StringType(), True),\n",
    "                              StructField('ProductCost', DoubleType(), True),\n",
    "                              StructField('ProductPrice', DoubleType(), True)])\n",
    "\n",
    "# 4. Read the data from a CSV file, with user defined schema into a dataframe products_df\n",
    "products_df = (spark\n",
    "              .read\n",
    "              .format(file_type)\n",
    "              .option(\"inferSchema\", infer_schema)\n",
    "              .schema(products_schema)\n",
    "              .option(\"header\", first_row_is_header)\n",
    "              .option(\"sep\", delimiter)\n",
    "              .load(products_file_location))\n",
    "\n",
    "# 5. Register products_df DataFrame as a Temporary View\n",
    "products_df.createOrReplaceTempView(\"products\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a0ec111-a405-4f70-9d32-4b19953f05e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------\n",
    "# 4. Read product_categories.csv file located in the distributed file system\n",
    "# --------------------------------------------------------------------------\n",
    "# 1. File location\n",
    "product_categories_file_location = \"dbfs:/FileStore/tables/Product_Categories.csv\"\n",
    "\n",
    "# 2. CSV options\n",
    "infer_schema = \"False\"\n",
    "first_row_is_header = \"True\"\n",
    "delimiter = \",\"\n",
    "\n",
    "# 3. Define the Schema for product_categories File\n",
    "product_categories_schema = StructType([ StructField('ProductCategoryKey', IntegerType(), nullable=False),\n",
    "                                         StructField('CategoryName', StringType(), True)])\n",
    "\n",
    "# 4. Read the data from a CSV file, with user defined schema into a dataframe product_categories_df\n",
    "product_categories_df = (spark\n",
    "                          .read\n",
    "                          .format(file_type)\n",
    "                          .option(\"inferSchema\", infer_schema)\n",
    "                          .schema(product_categories_schema)\n",
    "                          .option(\"header\", first_row_is_header)\n",
    "                          .option(\"sep\", delimiter)\n",
    "                          .load(product_categories_file_location))\n",
    "\n",
    "# 5. Register product_categories_df DataFrame as a Temporary View\n",
    "product_categories_df.createOrReplaceTempView('product_categories')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6af8bde7-3631-4205-af9b-ba08e86cff40",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# 5. Read product_subcategories.csv file located in the distributed file system\n",
    "# -----------------------------------------------------------------------------\n",
    "# 1. File location\n",
    "product_subcategories_file_location = \"dbfs:/FileStore/tables/Product_Subcategories.csv\"\n",
    "\n",
    "# 2. CSV options\n",
    "infer_schema = \"False\"\n",
    "first_row_is_header = \"True\"\n",
    "delimiter = \",\"\n",
    "\n",
    "# 3. Define the Schema for product_subcategories File\n",
    "product_subcategories_schema = StructType([ StructField('ProductSubcategoryKey', IntegerType(), nullable=False),\n",
    "                                            StructField('SubcategoryName', StringType(), True),\n",
    "                                            StructField('ProductCategoryKey', IntegerType(), True)])\n",
    "\n",
    "# 4. Read the data from a CSV file, with user defined schema into a dataframe product_subcategories_df\n",
    "product_subcategories_df = (spark\n",
    "                            .read\n",
    "                            .format(file_type)\n",
    "                            .option(\"inferSchema\", infer_schema)\n",
    "                            .schema(product_subcategories_schema)\n",
    "                            .option(\"header\", first_row_is_header)\n",
    "                            .option(\"sep\", delimiter)\n",
    "                            .load(product_subcategories_file_location))\n",
    "\n",
    "# 5. Register product_subcategories_df DataFrame as a Temporary View\n",
    "product_subcategories_df.createOrReplaceTempView('product_subcategories')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "110044c7-1176-42bd-895a-3fbe93b25ef8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------\n",
    "# 6. Read territories.csv file located in the distributed file system\n",
    "# -------------------------------------------------------------------\n",
    "# 1. File location\n",
    "territories_file_location = \"dbfs:/FileStore/tables/Territories.csv\"\n",
    "\n",
    "# 2. CSV options\n",
    "infer_schema = \"False\"\n",
    "first_row_is_header = \"True\"\n",
    "delimiter = \",\"\n",
    "\n",
    "# 3. Define the Schema for territories_file File\n",
    "territories_schema = StructType([ StructField('SalesTerritoryKey', IntegerType(), nullable=False),\n",
    "                                  StructField('Region', StringType(), True),\n",
    "                                  StructField('Country', StringType(), True),\n",
    "                                  StructField('Continent', StringType(), True)])\n",
    "\n",
    "# 4. Read the data from a CSV file, with user defined schema into a dataframe territories_df\n",
    "territories_df = (spark\n",
    "                  .read\n",
    "                  .format(file_type)\n",
    "                  .option(\"inferSchema\", infer_schema)\n",
    "                  .schema(territories_schema)\n",
    "                  .option(\"header\", first_row_is_header)\n",
    "                  .option(\"sep\", delimiter)\n",
    "                  .load(territories_file_location))\n",
    "\n",
    "# 5. Register territories_df DataFrame as a Temporary View\n",
    "territories_df.createOrReplaceTempView('territories')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60952cb7-3813-4adb-893e-7c6536ef8c7a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -----------------\n",
    "# 7. Transformation - Using SparkSQL \n",
    "# -----------------\n",
    "# Filter the Accessories category in the United States and rollup the Sales & Quantity at the Customer-level.\n",
    "\n",
    "bike_df_SQL = spark.sql(\n",
    "  \"\"\"\n",
    "  SELECT \n",
    "  S.CustomerKey,C.FirstName,C.LastName, PC.CategoryName, T.Country, \n",
    "  sum(ProductPrice * OrderQuantity) AS Total_Sales,\n",
    "  sum(OrderQuantity) AS Total_Quantity\n",
    "  FROM sales AS S\n",
    "  LEFT JOIN products AS P ON P.ProductKey = S.ProductKey\n",
    "  LEFT JOIN product_subcategories AS PS ON PS.ProductSubcategoryKey = P.ProductSubcategoryKey\n",
    "  LEFT JOIN product_categories AS PC ON PC.ProductCategoryKey = PS.ProductCategoryKey\n",
    "  LEFT JOIN Customers AS C ON C.CustomerKey = S.CustomerKey\n",
    "  LEFT JOIN territories AS T ON T.SalesTerritoryKey = S.TerritoryKey\n",
    "  WHERE CategoryName ='Accessories' AND Country = 'United States'\n",
    "  GROUP BY S.CustomerKey,C.FirstName,C.LastName, PC.CategoryName, T.Country \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8713d034-4632-4cbd-a5d8-80017e3451c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ------------\n",
    "# 8. Data Load\n",
    "# ------------\n",
    "# Processed Data is Loaded into the Databricks Distributed file System \n",
    "\n",
    "bike_df_SQL.coalesce(1)\\\n",
    ".write\\\n",
    ".mode(\"overwrite\")\\\n",
    ".option(\"header\", True)\\\n",
    ".option(\"compression\", \"gzip\")\\\n",
    ".csv('dbfs:/user/g.k.lohit@gmail.com')"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "Workflow using SparkSQL",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
